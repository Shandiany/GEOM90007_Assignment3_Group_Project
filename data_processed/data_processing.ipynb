{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "24f1e2e4",
      "metadata": {},
      "source": [
        "# Data Processing Notebook (Python)\n",
        "Project: GEOM90007 Assignment 3 – Group Project\\n\\n\n",
        "This notebook reads all CSV files under `data/raw/`, applies dataset-specific filters,\\n\n",
        "and writes analysis-ready outputs to `data/processed/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d9935860",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pandas 2.2.2\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_columns', 120)\n",
        "print('pandas', pd.__version__)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "144d34ba",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(PosixPath('/Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/raw'),\n",
              " PosixPath('/Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed'))"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Paths (robust to running from repo root or data/)\n",
        "ROOT = Path('.').resolve()\n",
        "candidates = [\n",
        "    ROOT / 'data',           # notebook at repo root\n",
        "    ROOT,                    # notebook already inside data/\n",
        "    ROOT.parent / 'data'     # fallback: run from subfolder\n",
        "]\n",
        "DATA = None\n",
        "for c in candidates:\n",
        "    if (c / 'raw').exists():\n",
        "        DATA = c\n",
        "        break\n",
        "if DATA is None:\n",
        "    raise FileNotFoundError(f\"Could not locate 'data/raw' from {ROOT}\")\n",
        "\n",
        "RAW = DATA / 'raw'\n",
        "PROCESSED = DATA / 'processed'\n",
        "\n",
        "# Do not create RAW; it must already exist in the repo.\n",
        "if not (RAW.exists() and RAW.is_dir()):\n",
        "    raise FileNotFoundError(f\"Expected existing raw folder at {RAW}\")\n",
        "# Ensure only output folders exist\n",
        "PROCESSED.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "RAW, PROCESSED\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35968900",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Helpers\n",
        "def snake(s: str) -> str:\n",
        "    s = re.sub(r'[^0-9a-zA-Z]+', '_', s.strip())\n",
        "    s = re.sub(r'_+', '_', s)\n",
        "    return s.strip('_').lower()\n",
        "\n",
        "def clean_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.rename(columns=lambda c: snake(str(c)))\n",
        "\n",
        "def read_csv_any(path: Path) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path)\n",
        "    except UnicodeDecodeError:\n",
        "        return pd.read_csv(path, encoding='latin-1')\n",
        "\n",
        "def find_col(df: pd.DataFrame, *keys, default=None):\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    for k in keys:\n",
        "        for name, orig in cols.items():\n",
        "            if k in name:\n",
        "                return orig\n",
        "    return default\n",
        "\n",
        "def ensure_datetime(df: pd.DataFrame, prefer_month=True):\n",
        "    # Try common datetime columns\n",
        "    cands = ['date_time','datetime','timestamp','date','period','month','time']\n",
        "    for c in cands:\n",
        "        col = find_col(df, c)\n",
        "        if col is not None:\n",
        "            dt = pd.to_datetime(df[col], errors='coerce')\n",
        "            if dt.notna().any():\n",
        "                return dt\n",
        "    # Try year/month/day combos\n",
        "    y = find_col(df, 'year')\n",
        "    m = find_col(df, 'month')\n",
        "    d = find_col(df, 'day')\n",
        "    if y is not None and m is not None:\n",
        "        day = df[d] if d else 1\n",
        "        try:\n",
        "            return pd.to_datetime(dict(year=df[y], month=df[m], day=day), errors='coerce')\n",
        "        except Exception:\n",
        "            pass\n",
        "    return pd.Series(pd.NaT, index=df.index)\n",
        "\n",
        "def between_dates(df: pd.DataFrame, start, end) -> pd.DataFrame:\n",
        "    dt = ensure_datetime(df)\n",
        "    mask = (dt >= pd.to_datetime(start)) & (dt <= pd.to_datetime(end))\n",
        "    out = df.loc[mask].copy()\n",
        "    out['__dt'] = dt[mask]\n",
        "    return out\n",
        "\n",
        "def write_out(df: pd.DataFrame, name: str):\n",
        "    path = PROCESSED / name\n",
        "    df.to_csv(path, index=False)\n",
        "    print('Wrote', path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9e3a24c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Discover raw files\n",
        "print('Using RAW folder:', RAW)\n",
        "raw_files = {f.name: f for f in sorted(RAW.glob('*.csv'))}\n",
        "print('Found CSV files:', len(raw_files))\n",
        "list(raw_files.keys())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be0e55e0",
      "metadata": {},
      "source": [
        "## Time Window\n",
        "We will repeatedly use this window for filtering: 2024-10 to 2025-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff1eed9c",
      "metadata": {},
      "outputs": [],
      "source": [
        "START = '2024-10-01'\n",
        "END   = '2025-10-31'\n",
        "START, END\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "929225f2",
      "metadata": {},
      "source": [
        "## 1) exchange_rates_monthly.csv\n",
        "- Keep months 2024-10 .. 2025-10 inclusive.\n",
        "- Keep currencies: CNY, NZD, INR, SGD, HKD, MYR, JPY, USD, GBP, IDR.\n",
        "- Output tidy columns: date, currency, rate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "2fc5f7d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/exchange_rates_2024_9_to_2025_9.csv\n"
          ]
        }
      ],
      "source": [
        "ex_path = raw_files.get('exchange_rates_monthly.csv')\n",
        "\n",
        "ex = clean_columns(read_csv_any(ex_path))\n",
        "currencies = ['units', 'usd','cny','nzd','inr','sgd','hkd','myr','jpy','gbp','idr']\n",
        "ex = ex[currencies]\n",
        "write_out(ex, 'exchange_rates_2024_9_to_2025_9.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0ce7707",
      "metadata": {},
      "source": [
        "## 2) fares_for_bi.csv\n",
        "- Keep 2020–2025 and rows related to Melbourne."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c4e88b59",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/fares_for_bi_2020_2025_melbourne.csv\n"
          ]
        }
      ],
      "source": [
        "fares_path = raw_files.get('fares_for_bi.csv')\n",
        "\n",
        "fr = clean_columns(read_csv_any(fares_path))\n",
        "\n",
        "# Keep years 2020–2025\n",
        "fr_filtered = fr[(fr['year'] >= 2020) & (fr['year'] <= 2025)].copy()\n",
        "\n",
        "# Keep rows containing \"melbourne\" in any text column\n",
        "text_cols = [c for c in fr_filtered.columns if fr_filtered[c].dtype == 'object']\n",
        "melb_mask = pd.Series(False, index=fr_filtered.index)\n",
        "for c in text_cols:\n",
        "    melb_mask |= fr_filtered[c].astype(str).str.contains('melbourne', case=False, na=False)\n",
        "\n",
        "fr_melb = fr_filtered[melb_mask].copy()\n",
        "write_out(fr_melb, 'fares_for_bi_2020_2025_melbourne.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d6943a",
      "metadata": {},
      "source": [
        "## 3) landmarks_and_places_...csv\n",
        "- Keep only selected Sub Theme categories (fuzzy matching)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "89411f9b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/landmarks_selected_subthemes.csv\n"
          ]
        }
      ],
      "source": [
        "lm_name = 'landmarks_and_places_of_interest_including_schools_theatres_health_services_spor.csv'\n",
        "lm_path = raw_files.get(lm_name)\n",
        "\n",
        "targets = [\n",
        "    'Art Gallery / Museum',\n",
        "    'Church',\n",
        "    'Function / Conference / Exhibition Centre',\n",
        "    'Informal Outdoor Facility (Park / Garden / Reserve)',\n",
        "    'Major Sports & Recreation Facility',\n",
        "    'Theatre / Live Entertainment Venue',\n",
        "    'Tertiary (University)'\n",
        "]\n",
        "\n",
        "\n",
        "lm = clean_columns(read_csv_any(lm_path))\n",
        "\n",
        "if 'sub_theme' in lm.columns:\n",
        "    lm_selected = lm[lm['sub_theme'].isin(targets)].copy()\n",
        "    write_out(lm_selected, 'landmarks_selected_subthemes.csv')\n",
        "else:\n",
        "    print('sub_theme column not found — writing cleaned copy only.')\n",
        "    write_out(lm, 'landmarks_clean.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a487dee0",
      "metadata": {},
      "source": [
        "## 4) pedestrian_counting_system_monthly_counts_per_hour.csv\n",
        "- Keep 2024-10 .. 2025-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3a03bf9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/pedestrian_counts_2024_10_to_2025_10.csv\n"
          ]
        }
      ],
      "source": [
        "ped_path = raw_files.get('pedestrian_counting_system_monthly_counts_per_hour.csv')\n",
        "ped = clean_columns(read_csv_any(ped_path))\n",
        "\n",
        "# Parse sensing_date directly\n",
        "ped['__dt'] = pd.to_datetime(ped['sensing_date'], errors='coerce')\n",
        "\n",
        "# Filter by date range\n",
        "ped_f = ped[(ped['__dt'] >= pd.to_datetime(START)) &\n",
        "            (ped['__dt'] <= pd.to_datetime(END))].copy()\n",
        "\n",
        "write_out(ped_f.drop(columns=['__dt']), 'pedestrian_counts_2024_10_to_2025_10.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5ddb58ae",
      "metadata": {},
      "source": [
        "## 5) weather_microclimate_sensors_data.csv\n",
        "- Keep 2024-10 .. 2025-10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d370ab79",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/weather_microclimate_daily_avg.csv\n"
          ]
        }
      ],
      "source": [
        "wx_path = raw_files.get('weather_microclimate_sensors_data.csv')\n",
        "\n",
        "wx = clean_columns(read_csv_any(wx_path))\n",
        "sensors = ['aws5-0999', 'ICTMicroclimate-11', 'ICTMicroclimate-09', 'ICTMicroclimate-07']\n",
        "wx = wx[wx[\"device_id\"].astype(str).str.strip().isin(sensors)].copy()\n",
        "\n",
        "# Step 1: convert time to Melbourne TZ and drop timezone\n",
        "wx['Target_Time'] = (\n",
        "    pd.to_datetime(wx['time'], utc=True)\n",
        "      .dt.tz_convert('Australia/Melbourne')\n",
        "      .dt.tz_localize(None)\n",
        ")\n",
        "\n",
        "# Step 2: filter by date window\n",
        "wx = wx[(wx['Target_Time'] >= pd.to_datetime(START)) &\n",
        "        (wx['Target_Time'] <= pd.to_datetime(END))].copy()\n",
        "\n",
        "# Step 3: aggregate by date\n",
        "wx['Target_Date'] = wx['Target_Time'].dt.date\n",
        "agg_cols = [\n",
        "    'minimumwinddirection', 'averagewinddirection', 'maximumwinddirection',\n",
        "    'minimumwindspeed', 'averagewindspeed', 'gustwindspeed',\n",
        "    'airtemperature', 'relativehumidity', 'atmosphericpressure',\n",
        "    'pm25', 'pm10', 'noise'\n",
        "]\n",
        "\n",
        "wx_daily = (\n",
        "    wx.groupby(['device_id', 'sensorlocation', 'latlong', 'Target_Date'])[agg_cols]\n",
        "      .mean()\n",
        "      .reset_index()\n",
        ")\n",
        "\n",
        "# Step 4: format as string for output\n",
        "wx_daily['Target_Time'] = pd.to_datetime(wx_daily['Target_Date']).dt.strftime('%Y-%m-%d %H:%M:%S')\n",
        "wx_daily = wx_daily.drop(columns=['Target_Date'])\n",
        "\n",
        "write_out(wx_daily, 'weather_microclimate_daily_avg.csv')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "254335b2",
      "metadata": {},
      "source": [
        "## 6) short_term_visitor_arrivals,_state_of_stay_2025.csv\n",
        "- Generic cleaning for analysis (no special filter specified)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8727ac52",
      "metadata": {},
      "outputs": [],
      "source": [
        "st_name = 'short_term_visitor_arrivals,_state_of_stay_2025.csv'\n",
        "st_path = raw_files.get(st_name)\n",
        "\n",
        "st = clean_columns(read_csv_any(st_path))\n",
        "# Basic tidying: remove empty cols/rows, attempt datetime parse for any date-like col\n",
        "st = st.dropna(how='all').copy()\n",
        "# If monthly columns present, try to build a datetime index\n",
        "dt = ensure_datetime(st)\n",
        "if dt.notna().any():\n",
        "    st['date'] = dt\n",
        "write_out(st, 'short_term_visitor_arrivals_clean.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "5cb16ae0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/cleaned_short_term_visitor_arrivals,_state_of_stay_2019_2025.csv\n"
          ]
        }
      ],
      "source": [
        "st_name = 'short_term_visitor_arrivals,_state_of_stay_2019_2025.csv'\n",
        "st_path = raw_files.get(st_name)\n",
        "\n",
        "st = clean_columns(read_csv_any(st_path))\n",
        "\n",
        "\n",
        "# Rename columns\n",
        "df = st.rename(columns={\n",
        "    'state_or_territory_of_stay': 'state',\n",
        "    'aug_2019_no': 'Aug_2019',\n",
        "    'aug_2023_no': 'Aug_2023',\n",
        "    'aug_2024_no': 'Aug_2024',\n",
        "    'aug_2025_no': 'Aug_2025',\n",
        "    'aug_2024_to_aug_2025_change': 'change_pct'\n",
        "})\n",
        "\n",
        "# Remove commas and convert to numeric\n",
        "num_cols = ['Aug_2019','Aug_2023','Aug_2024','Aug_2025','change_pct']\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].astype(str).str.replace(',', '').str.replace('\"', '').astype(float)\n",
        "\n",
        "# Melt to long format\n",
        "df_long = df.melt(\n",
        "    id_vars=['state', 'change_pct'],\n",
        "    value_vars=['Aug_2019','Aug_2023','Aug_2024','Aug_2025'],\n",
        "    var_name='period',\n",
        "    value_name='value'\n",
        ")\n",
        "\n",
        "# Extract month & year\n",
        "df_long['month'] = df_long['period'].str.extract(r'([A-Za-z]+)')\n",
        "df_long['year'] = df_long['period'].str.extract(r'(\\d{4})').astype(int)\n",
        "\n",
        "# Reorder columns\n",
        "df_long = df_long[['state','year','month','value','change_pct']].sort_values(['state','year'])\n",
        "\n",
        "write_out(df_long, 'cleaned_short_term_visitor_arrivals,_state_of_stay_2019_2025.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "id": "0f432cbe",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wrote /Users/wangwang/Desktop/School/GEO90007/GEOM90007_Assignment3_Group_Project/data/processed/cleaned_short_term_visitor_arrivals_Vic_top_10_source_countries.csv\n"
          ]
        }
      ],
      "source": [
        "st_name = 'short_term_visitor_arrivals_Vic_top_10_source_countries.csv'\n",
        "st_path = raw_files.get(st_name)\n",
        "\n",
        "st = clean_columns(read_csv_any(st_path))\n",
        "\n",
        "\n",
        "# Rename columns\n",
        "df = st.rename(columns={\n",
        "    'country_of_residence': 'Country',\n",
        "    'aug_19': 'Aug_2019',\n",
        "    'aug_23': 'Aug_2023',\n",
        "    'aug_24': 'Aug_2024',\n",
        "    'aug_25': 'Aug_2025'})\n",
        "# Remove commas and convert to numeric\n",
        "num_cols = ['Aug_2019','Aug_2023','Aug_2024','Aug_2025']\n",
        "for c in num_cols:\n",
        "    df[c] = df[c].astype(str).str.replace(',', '').str.replace('\"', '').astype(float)\n",
        "\n",
        "\n",
        "write_out(df, 'cleaned_short_term_visitor_arrivals_Vic_top_10_source_countries.csv')\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bbff314",
      "metadata": {},
      "source": [
        "### Done\n",
        "Processed datasets are saved under `data/processed/`."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
